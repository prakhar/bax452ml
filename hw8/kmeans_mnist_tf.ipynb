{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW8\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data ingestion](#Data-ingestion)\n",
    "  3. [Data inspection](#Data-inspection)\n",
    "  4. [Data conversion](#Data-conversion)\n",
    "3. [Training the K-Means model](#Training-the-K-Means-model)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "MNIST database classfication using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='sagemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training.  In this case we'll use the MNIST dataset, which contains 70K 28 x 28 pixel images of handwritten digits.  For more details, please see [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 s, sys: 480 ms, total: 2.71 s\n",
      "Wall time: 3.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJUAAACfCAYAAADwOZspAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nihgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f38c026a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (2,10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "show_digit(train_set[0][30], 'This is a {}'.format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.t2.medium')\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                             instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking and Deleting endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1, len(data)], dtype=tf.float32)\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response['outputs']['classes']['int64Val'][0]\n",
    "    print(\"prediction is {}\".format(prediction))\n",
    "    sagemaker.Session().delete_endpoint(mnist_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
